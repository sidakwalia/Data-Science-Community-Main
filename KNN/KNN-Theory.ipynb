{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application Flow \n",
    "k-NN is one of the most fundamental algorithms for classification and regression in the Machine Learning world.\n",
    "\n",
    "But before proceeding with the algorithm, let’s first discuss the lifecycle of any machine learning model. This diagram explains the creation of a Machine Learning model from scratch and then taking the same model further with hyperparameter tuning to increase its accuracy, deciding the deployment strategies for that model and once deployed setting up the logging and monitoring frameworks to generate reports and dashboards based on the client requirements. \n",
    "A typical lifecycle diagram for a machine learning model looks like:\n",
    "\n",
    "<img src=\"MLApplicationFlow.PNG\" width= \"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-Nearest Neighbors\n",
    "\n",
    "K-nearest neighbors (KNN) is a type of supervised learning algorithm which is used for both regression and classification purposes, but mostly it is used for the later. Given a dataset with different classes, KNN tries to predict the correct class of test data by calculating the distance between the test data and all the training points. It then selects the k points which are closest to the test data.  Once the points are selected, the algorithm calculates the probability (in case of classification) of the test point belonging to the classes of the k training points and the class with the highest probability is selected. In the case of a regression problem, the predicted value is the mean of the k selected training points.\n",
    "\n",
    "\n",
    "Let’s understand this with an illustration:\n",
    "\n",
    "\n",
    "1)\tGiven a training dataset as given below. We have a new test data that we need to assign to one of the two classes.\n",
    "\n",
    "<img src=\"1.png\" width=\"\">\n",
    "                                      \n",
    "\n",
    "2)\tNow, the k-NN algorithm calculates the distance between the test data and the given training data.\n",
    "\n",
    "<img src=\"2.png\" width=\"\">\n",
    "\n",
    "                                                        \n",
    "3)\tAfter calculating the distance, it will select the k training points which are nearest to the test data. Let’s assume the value of k is 3 for our example.\n",
    "\n",
    "<img src=\"3.png\" width=\"\">                                            \n",
    "\n",
    "\n",
    "4)\tNow, 3 nearest neighbors are selected, as shown in the figure above. Let’s see in which class our test data will be assigned :\n",
    "\n",
    "Number of Green class values = 2\n",
    "Number of Red class values = 1\n",
    "Probability(Green) = 2/3\n",
    "Probability(Red) = 1/3\n",
    "\n",
    "Since the probability for Green class is higher than Red, the k-NN algorithm will assign the test data to the Green class.\n",
    "\n",
    "Similarly, if this were the case of a regression problem, the predicted value for the test data will simply be the mean of all the 3 nearest values.\n",
    "\n",
    "This is the basic working algorithm for k-NN. Let’s understand how the distance is calculated :\n",
    "\n",
    "### Euclidean Distance: \n",
    "\n",
    "It is the most commonly used method to calculate the distance between two points.\n",
    "The Euclidean distance between two points ‘p(p1,p2)’ and ‘q(q1,q2)’ is calculated  as :\n",
    "\n",
    "<img src=\"4.png\" width=\"\">       image source : Wikipedia\n",
    "\n",
    "<img src=\"5.png\" width=\"\">\n",
    "\n",
    "                                          \n",
    "Similarly,for n-dimensional space, the Euclidean distance is given as :\n",
    "\n",
    "<img src=\"6.png\" width=\"\">\n",
    " \n",
    "### Hamming distance\n",
    "A/c to Wikipedia, hamming distance is a distance metric that measures the number of mismatches between two vectors. It is mostly used in the case of categorical data.\n",
    "\n",
    "<img src=\"7.png\" width=\"\">\n",
    "                                                               \n",
    "Generally, if we have features as categorical data then we consider the difference to be 0 if both the values are the same and the difference is 1 if both the values are different.\n",
    "\n",
    "### Manhattan Distance\n",
    "A/c to Wikipedia, The Manhattan distance, also known as L1 norm, Taxicab norm, Rectilinear distance or City block distance. This distance represents the sum of the absolute differences between the opposite values in vectors.\n",
    "\n",
    "<img src=\"8.png\" width=\"\">\n",
    "                                                           \n",
    "Manhattan Distance is less influenced by outliers than the Euclidean distance. With very high dimensional data it is more preferred. \n",
    "\n",
    "### Lazy Learners\n",
    "\n",
    "k-NN algorithms are often termed as Lazy learners. Let’s understand why is that. Most of the algorithms like Bayesian classification, logistic regression, SVM etc., are called Eager learners. These algorithms generalize over the training set before receiving the test data i.e. they create a model based on the training data before receiving the test data and then do the prediction/classification on the test data.\n",
    "But this is not the case with the k-NN algorithm. It doesn’t create a generalized model for the training set but waits for the test data. Once test data is provided then only it starts generalizing the training data to classify the test data.  So, a lazy learner just stores the training data and waits for the test set. Such algorithms work less while training and more while classifying a given test dataset.\n",
    "\n",
    "### Weighted Nearest Neighbor\n",
    "In weighted k-NN, we assign weights to the k nearest neighbors.The weights are typically assigned on the basis of distance.\n",
    "Sometimes rest of data points are assigned a weight of 0 also. The main intuition is that the points in neighbor should have more weights than father points.\n",
    "\n",
    "\n",
    "\n",
    "### Choosing the value of k\n",
    "The value of k affects the k-NN classifier drastically. The flexibility of the model decreases with the increase of ‘k’.  With lower value of ‘k’ variance is high and bias is low but as we increase the value of ‘k’ variance starts decreasing and bias starts increasing. With very low values of ‘k’ there is a chance of algorithm overfitting the data whereas with very high value of ‘k’ there is a chance of underfitting. \n",
    "Let’s visualize the trade-off between ‘1/k’, train error rate and test error rate:\n",
    "\n",
    "<img src=\"9.png\" width=\"\">  image source: “ISLR”\n",
    "\n",
    "We can clearly see that the train error rate increases with the increase in the value of ‘k’ whereas test error rate decreases initially and then increases again.  So, our goal should be to choose such value of ‘k’ for which we get a minimum of both the errors and avoid overfitting as well as underfitting.\n",
    "We use different ways to calculate the optimum value of ‘k’ such as cross validation, error versus k curve, checking accuracy for each value of ‘k’ etc. \n",
    "\n",
    "### Pros and Cons of k-NN Algorithm\n",
    "\n",
    "Pros:\n",
    "*\tIt can be used for both regression and classification problems.\n",
    "*\tIt is very simple and easy to implement.\n",
    "*\tMathematics behind the algorithm is easy to understand.\n",
    "*\tThere is no need to create model or do hyperparameter tuning.\n",
    "*   KNN doesn't make any assumption for the distribution of the given data.\n",
    "*   There is not much time cost in training phase.\n",
    "\n",
    "Cons:\n",
    "*\tFinding the optimum value of ‘k’\n",
    "*\tIt takes a lot of time to compute the distance between each test sample and all training samples.\n",
    "*\tSince the model is not saved beforehand in this algorithm (lazy learner), so every time one predicts a test value, it follows the same steps again and again. \n",
    "*\tSince, we need to store the whole training set for every test set, it requires a lot of space.\n",
    "*\tIt is not suitable for high dimensional data.\n",
    "*   Expensive in testing phase\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different ways to perform k-NN\n",
    "\n",
    "Above we studied the way k-NN classifies the data by calculating the distance of test data from each of the observations and selecting ‘k’ values. This approach is also known as “Brute Force k-NN”.  This is computionally very expensive. So, there are other ways as well to perfrom k-NN which are comparatively less expensive than Brute force approach. The idea behind using other algorithms for k-NN classifier is to reduce the time during test period by preprocessing the training data in such a way that the test data can be easily classified in the appropriate clusters.\n",
    "\n",
    "\n",
    "Let’s discuss and understand the two most famous algorithms:\n",
    "\n",
    "#### k-Dimensional Tree (kd tree)\n",
    "\n",
    "k-d tree is a hierarchical binary tree. When this algorithm is used for k-NN classficaition, it rearranges the whole dataset in a binary tree structure, so that when test data is provided, it would give out the result by traversing through the tree, which takes less time than brute search.\n",
    "\n",
    "<img src=\"10.png\" width=\"\"> \n",
    " \n",
    "The dataset is divided like a tree as shown in the above figure. Say we have 3 dimensional data i.e. (x,y,z) then the tree is formed with root node being one of the dimensions, here we start with ‘x’. Then on the next level the split is done on basis of the second dimension, ‘y’ in our case. Similarly, third level with 3rd dimension and so on.  And in case of ‘k’ dimensions, each split is made on basis of ‘k’ dimensions. \n",
    "Let’s understand how k-d trees are formed with an example:\n",
    "\n",
    "<img src=\"11.png\" width=\"\"> \n",
    "\n",
    "<img src=\"12.png\" width=\"\"> \n",
    "\n",
    "<img src=\"13.png\" width=\"\"> \n",
    "\n",
    "<img src=\"14.png\" width=\"\"> \n",
    " \n",
    " \n",
    "\n",
    "Once the tree is formed , it is easy for algorithm to search for the probable nearest neighbor just by traversing the tree.  The main problem k-d trees is that it gives probable nearest neighbors but can miss out actual nearest neighbors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will see the implementation of all the above concepts in python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Validation\n",
    "\n",
    "Suppose you train a model on a given dataset using any specific algorithm. You tried to find the accuracy of the trained model using the same training data and found the accuracy to be 95% or maybe even 100%. What does this mean? Is your model ready for prediction? The answer is no.\n",
    "Why? Because your model has trained itself on the given data, i.e. it knows the data and it has generalized over it very well. But when you try and predict over a new set of data, it’s most likely to give you very bad accuracy, because it has never seen the data before and thus it fails to generalizes well over it. This is the problem of overfitting. \n",
    "To tackle such problem, Cross-validation comes into the picture. Cross-validation is a resampling technique with a basic idea of dividing the training dataset into two parts i.e. train and test. On one part(train) you try to train the model and on the second part(test) i.e. the data which is unseen for the model, you make the prediction and check how well your model works on it. If the model works with good accuracy on your test data, it means that the model has not overfitted the training data and can be trusted with the prediction, whereas if it performs with bad accuracy then our model is not to be trusted and we need to tweak our algorithm.\n",
    "\n",
    "\n",
    "Let’s see the different approaches of Cross-Validation:\n",
    "\n",
    "*\tHold Out Method: \n",
    "\n",
    "It is the most basic of the CV techniques. It simply divides the dataset into two sets of training and test. The training dataset is used to train the model and then test data is fitted in the trained model to make predictions. We check the accuracy and assess our model on that basis. This method is used as it is computationally less costly. But the evaluation based on the Hold-out set can have a high variance because it depends heavily on which data points end up in the training set and which in test data. The evaluation will be different every time this division changes.\n",
    "\n",
    "*\tk-fold Cross-Validation\n",
    "\n",
    "<img src=\"cv1.png\" width=\"\"> \n",
    "\n",
    " img_src:Wikipedia\n",
    "\n",
    "To tackle the high variance of Hold-out method, the k-fold method is used. The idea is simple, divide the whole dataset into ‘k’ sets preferably of equal sizes. Then the first set is selected as the test set and the rest ‘k-1’ sets are used to train the data. Error is calculated for this particular dataset.\n",
    "Then the steps are repeated, i.e. the second set is selected as the test data, and the remaining ‘k-1’ sets are used as the training data. Again, the error is calculated. Similarly, the process continues for ‘k’ times. In the end, the CV error is given as the mean of the total errors calculated individually, mathematically given as:\n",
    "\n",
    "<img src=\"cv2.png\" width=\"\"> \n",
    "                                               \n",
    "The variance in error decreases with the increase in ‘k’. The disadvantage of k-fold cv is that it is computationally expensive as the algorithm runs from scratch for ‘k’ times.\n",
    "\n",
    "*  Leave One Out Cross Validation (LOOCV)\n",
    "\n",
    "<img src=\"cv3.png\" width=\"\"> \n",
    " \n",
    "LOOCV is a special case of k-fold CV, where k becomes equal to n (number of observations). So instead of creating two subsets, it selects a single observation as a test data and rest of data as the training data. The error is calculated for this test observations. Now, the second observation is selected as test data, and the rest of the data is used as the training set. Again, the error is calculated for this particular test observation. This process continues ‘n’ times and in the end, CV error is calculated as:\n",
    "\n",
    "<img src=\"cv4.png\" width=\"\"> \n",
    "                                             \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
